{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fVnj08ZRl-I9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Data"
   ],
   "metadata": {
    "id": "B2APRzR5naUZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ],
   "metadata": {
    "id": "CVd2VrtrmZ87"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text=open(path_to_file,'rb').read().decode(encoding='utf8')"
   ],
   "metadata": {
    "id": "o-CfoWHwmrJc"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Length of text :{len(text)} characters')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVqSUtdlm1hN",
    "outputId": "d3a53db2-6b6e-4cc8-d71f-ba667b200041"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of text :1115394 characters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(text[:250])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMuiPn4Om_Hn",
    "outputId": "8eb11dc8-1a2a-4292-e0fb-c50dd86fb3dd"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab=sorted(set(text))\n",
    "print(f'{len(vocab)} unique caracters')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEkjB-e2noKM",
    "outputId": "19fa68d3-c523-4330-86ec-cff4094e3f17"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65 unique caracters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the text"
   ],
   "metadata": {
    "id": "xB6Mpk9ynhQ9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorize the text"
   ],
   "metadata": {
    "id": "DVyTV8ZWn7pb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_texts=['abcdefg','xyz']\n",
    "chars=tf.strings.unicode_split(example_texts,input_encoding='UTF-8')\n",
    "chars"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXG_hwZHnli8",
    "outputId": "487c61a7-f9aa-47a3-9eac-a40d2213ad0c"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab),mask_token=None)"
   ],
   "metadata": {
    "id": "kfC_WFltoVJk"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ids=ids_from_chars(chars)\n",
    "ids"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbFwAEfmqdLj",
    "outputId": "e2363bc5-b215-430a-a696-16b55c831f8d"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "chars_from_ids=tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),mask_token=None,invert=True)"
   ],
   "metadata": {
    "id": "1U21PnajpyyP"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "chars=chars_from_ids(ids)\n",
    "chars"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4bT8eWoqZic",
    "outputId": "befdb28b-a9a8-4ee1-f6d0-466aa9b863ca"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tf.strings.reduce_join(chars,axis=-1).numpy()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PtWHkDQrc_6",
    "outputId": "63b6b383-1163-43d1-becc-8cb42b2f1178"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids),axis=-1)"
   ],
   "metadata": {
    "id": "oGbLF-irrp5K"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tf.strings.unicode_split(text,'UTF-8')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzNnEHu6sk80",
    "outputId": "ca1ec070-3eff-4c88-c6e1-07085cab266e"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=string, numpy=array([b'F', b'i', b'r', ..., b'g', b'.', b'\\n'], dtype=object)>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_ids=ids_from_chars(tf.strings.unicode_split(text,'UTF-8'))\n",
    "all_ids"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1Wyo3zNsJzS",
    "outputId": "7f053497-dc16-48eb-c586-1971ac8c6435"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sequence_length=100"
   ],
   "metadata": {
    "id": "yh-vr9FKvwWh"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ids_dataset=tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "  print(chars_from_ids(ids))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPf5Hjq6ukSa",
    "outputId": "630b3849-a507-46b1-e195-ef1632942b05"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(b'F', shape=(), dtype=string)\n",
      "tf.Tensor(b'i', shape=(), dtype=string)\n",
      "tf.Tensor(b'r', shape=(), dtype=string)\n",
      "tf.Tensor(b's', shape=(), dtype=string)\n",
      "tf.Tensor(b't', shape=(), dtype=string)\n",
      "tf.Tensor(b' ', shape=(), dtype=string)\n",
      "tf.Tensor(b'C', shape=(), dtype=string)\n",
      "tf.Tensor(b'i', shape=(), dtype=string)\n",
      "tf.Tensor(b't', shape=(), dtype=string)\n",
      "tf.Tensor(b'i', shape=(), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sequences=ids_dataset.batch(sequence_length+1,drop_remainder=True)\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sv_mbx9GvRA6",
    "outputId": "ad9b3124-4210-4175-ea08-95230d582286"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for seq in sequences.take(1):\n",
    "  print(text_from_ids(seq).numpy())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGFUOELLxnNg",
    "outputId": "17affb6d-693f-4d6e-b92e-25d36ed99168"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess the Text"
   ],
   "metadata": {
    "id": "8kMp5VtLDGfR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ],
   "metadata": {
    "id": "W-ATuPYfypBo"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset=sequences.map(split_input_target)"
   ],
   "metadata": {
    "id": "-vsqjCGnyqpG"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for input_example,output_example in dataset.take(1):\n",
    "  print(f'Input : {text_from_ids(input_example)}')\n",
    "  print(f'Output : {text_from_ids(output_example)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSFKdV1Cy5qG",
    "outputId": "46b9666a-ed65-47b3-8449-2d007dc72bf8"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Output : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "BATCH_SIZE=64\n",
    "BUFFER_SIZE=10000\n",
    "\n",
    "dataset=(\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE,drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "dataset"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqWaAAnW0wz6",
    "outputId": "b706a797-ea81-4bdc-aee0-d76882aba4b3"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "id": "RDCTSRJ_DBGb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dim=256\n",
    "vocab_size=len(ids_from_chars.get_vocabulary())\n",
    "rnn_units=1024"
   ],
   "metadata": {
    "id": "h_R0gew_DQLR"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "  @tf.function()\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n"
   ],
   "metadata": {
    "id": "S6H7PmoY5SPf"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model=MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,)"
   ],
   "metadata": {
    "id": "r_hmqwl4CmNf"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw9SZ5kKCdR5",
    "outputId": "3d3f54b1-8ef1-49d3-ae0f-50a77daa61fe"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogIlOAN2P22y",
    "outputId": "450c39b1-6ead-4e0b-cc4b-f60a321212ef"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eB2tIkp0QhPq",
    "outputId": "8eee7ab4-5edd-4222-904f-da0a2c42aba7"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uo-YDEG-Qr2J",
    "outputId": "6eafc18f-d598-47a7-ff3a-9313e55b87a4"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input:\n",
      " b\"k wherein my soul recorded\\nThe history of all her secret thoughts:\\nSo smooth he daub'd his vice with\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b'ipzhQWdR!doDem[UNK]!zaQLA\\n-reLP??;q3z-pJ?TBpiehUiEKIf3Y3KGS.:!-CzibWyQ;psdbFIksridSWFE:r &au3zdWeTOi$YWP'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ],
   "metadata": {
    "id": "agpkdKJZSSVT"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGQyhHavSboP",
    "outputId": "ff3d109f-f009-4adc-cd55-72b3fdf0fc49"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1892157, shape=(), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJJ69oqqTBax",
    "outputId": "464b4c57-266d-4381-c4be-74ab3adc28b5"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "65.97103"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='adam',loss=loss)"
   ],
   "metadata": {
    "id": "rcy6IDpyqI0u"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint_dir='/content/training_checkpoints'\n",
    "checkpoint_prefix=os.path.join(checkpoint_dir,'ckpt_{epoch}')\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weight=True\n",
    ")"
   ],
   "metadata": {
    "id": "eAFFAJl0poNG"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs=30\n",
    "history=model.fit(dataset,epochs=epochs,callbacks=[checkpoint_callback])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKQjJPERqsOe",
    "outputId": "4ec5f994-9e18-4160-81e9-3b14ed9dcdb6"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 23s 83ms/step - loss: 2.7096\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 14s 74ms/step - loss: 1.9822\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 15s 74ms/step - loss: 1.7064\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 14s 73ms/step - loss: 1.5477\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.4500\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.3816\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 15s 75ms/step - loss: 1.3275\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 15s 71ms/step - loss: 1.2833\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 1.2406\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 15s 75ms/step - loss: 1.2008\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 14s 74ms/step - loss: 1.1593\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 15s 76ms/step - loss: 1.1177\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 1.0733\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 14s 74ms/step - loss: 1.0260\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.9769\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.9259\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 14s 75ms/step - loss: 0.8720\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 0.8194\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.7677\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 0.7198\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 15s 76ms/step - loss: 0.6736\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.6331\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 0.6006\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 15s 78ms/step - loss: 0.5698\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.5449\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.5253\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 14s 74ms/step - loss: 0.5075\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.4934\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 15s 74ms/step - loss: 0.4781\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 14s 72ms/step - loss: 0.4676\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Text"
   ],
   "metadata": {
    "id": "cGHe0vL5rvCL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "skip_ids = ids_from_chars(['[UNK]'])[:, None]\n",
    "skip_ids"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqcFrZpiswF1",
    "outputId": "2dd2f991-dbbe-47d6-c129-0a6def05470c"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]])>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "values=[-float('inf')]*len(skip_ids)"
   ],
   "metadata": {
    "id": "YqbCQP6t3xAi"
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "indices=skip_ids,\n",
    "dense_shape=[len(ids_from_chars.get_vocabulary())]"
   ],
   "metadata": {
    "id": "4eJWK1Vy6WDS"
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dense_shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWyYmMXl6a5W",
    "outputId": "08caef1e-6d8a-4861-cb06-87efe8da1b0c"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[66]"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])"
   ],
   "metadata": {
    "id": "w8483qv66iHl"
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "inputs=['ROMEO']\n",
    "input_chars = tf.strings.unicode_split(inputs, 'UTF-8')"
   ],
   "metadata": {
    "id": "pc7nPVhj--Fg"
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n",
    "\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "jgYWUxgbq7e5"
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ],
   "metadata": {
    "id": "SWtRLAh4Cg-t"
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start=time.time()\n",
    "states=None\n",
    "next_char=tf.constant(['ROMEO'])\n",
    "result=[next_char]\n",
    "states=None\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char,states=one_step_model.generate_one_step(next_char,states=states)\n",
    "  result.append(next_char)\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YdtDBFpCEuP",
    "outputId": "9b745a7b-11b0-4f36-ab90-b039260ee8d8"
   },
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROMEO:\n",
      "The arm of lions are full of death.\n",
      "Before I see the end of orph mine eyes aloud\n",
      "Which now you speak us unhappy? whence 'tis won:\n",
      "How may I call it: brother, there, be patient: I\n",
      "House or death: pray she might loathed toward sometimes,\n",
      "There rust his several peace thou offices' legs,\n",
      "The noble houses of tears and treason'd Richmond,\n",
      "So disvused with conscience and our hearts\n",
      "Will use to swell as cast, and I have heard of sen\n",
      "And keep the move of you, return'd,\n",
      "Can make the restless pine of my present pain,\n",
      "And let mild outrunness and this fash and stuff,\n",
      "Of the first word with one give thee roy\n",
      "To sea with an asurn to the end;\n",
      "And to my rights, to determine of this place,\n",
      "And this I cannot come there gentle Warwick,\n",
      "Let him be sent for your lass end you better.\n",
      "God pardon give no more behind that makes but sworn to the\n",
      "subject, such as you, and many faith of honour breath.\n",
      "\n",
      "EDWARD:\n",
      "Now, sir, this must be proud to hide his losality.\n",
      "I hear this knees, that we shall happy more respect\n",
      " \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.252020597457886\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CUSTOM Training"
   ],
   "metadata": {
    "id": "-1a4hOCPyCmS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnZ65PTeoXEp",
    "outputId": "5a32fad3-c60c-431b-9a26-b9eaefece105"
   },
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f3fc18db190>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomTraining(MyModel):\n",
    "\n",
    "  @tf.function\n",
    "  def train_on_step(self,inputs):\n",
    "    inputs,labels=inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions=self(inputs,training=True)\n",
    "      loss=self.loss(predictions,labels)\n",
    "    grad=tape.gradient(loss,self.trainable_varibales)\n",
    "    self.optimizer.apply_gradients(zip(grad,self.trainable_variables))\n",
    "\n",
    "    return {'loss':loss}\n"
   ],
   "metadata": {
    "id": "ONaF4K83pSiK"
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ],
   "metadata": {
    "id": "UM7kQPkPqnHB"
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(dataset, epochs=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZeCSY6-qrQ7",
    "outputId": "eac1fd1e-6517-40ef-d2dd-612daf12b556"
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "172/172 [==============================] - 15s 59ms/step - loss: 2.6996\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3fc0309ed0>"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "epochs=30\n",
    "\n",
    "mean=tf.metrics.Mean()\n",
    "for epoch in range(epochs):\n",
    "  start=time.time()\n",
    "  mean.reset_states()\n",
    "  for (batch_n,(inp,target)) in enumerate(dataset):\n",
    "    logs=model.train_step([inp,target])\n",
    "    mean.update_state(logs['loss'])\n",
    "    if batch_n % 50==0:\n",
    "      template=f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "      print(template)\n",
    "\n",
    "  if(epoch+1)%5==0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print()\n",
    "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "  print(\"_\"*80)\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PEUpa9lrAG_",
    "outputId": "6e321b4f-0989-467c-fdbf-2e19e4d52c16"
   },
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f3fc01104c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f3fc01104c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 2.6966\n",
      "Epoch 1 Batch 50 Loss 2.5645\n",
      "Epoch 1 Batch 100 Loss 2.4590\n",
      "Epoch 1 Batch 150 Loss 2.3708\n",
      "\n",
      "Epoch 1 Loss: 2.4971\n",
      "Time taken for 1 epoch 40.95 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 2.3358\n",
      "Epoch 2 Batch 50 Loss 2.2632\n",
      "Epoch 2 Batch 100 Loss 2.2006\n",
      "Epoch 2 Batch 150 Loss 2.1448\n",
      "\n",
      "Epoch 2 Loss: 2.2218\n",
      "Time taken for 1 epoch 26.20 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 2.1229\n",
      "Epoch 3 Batch 50 Loss 2.0741\n",
      "Epoch 3 Batch 100 Loss 2.0308\n",
      "Epoch 3 Batch 150 Loss 1.9922\n",
      "\n",
      "Epoch 3 Loss: 2.0453\n",
      "Time taken for 1 epoch 26.11 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.9765\n",
      "Epoch 4 Batch 50 Loss 1.9409\n",
      "Epoch 4 Batch 100 Loss 1.9097\n",
      "Epoch 4 Batch 150 Loss 1.8810\n",
      "\n",
      "Epoch 4 Loss: 1.9199\n",
      "Time taken for 1 epoch 40.95 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.8691\n",
      "Epoch 5 Batch 50 Loss 1.8424\n",
      "Epoch 5 Batch 100 Loss 1.8178\n",
      "Epoch 5 Batch 150 Loss 1.7957\n",
      "\n",
      "Epoch 5 Loss: 1.8259\n",
      "Time taken for 1 epoch 26.29 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.7865\n",
      "Epoch 6 Batch 50 Loss 1.7647\n",
      "Epoch 6 Batch 100 Loss 1.7453\n",
      "Epoch 6 Batch 150 Loss 1.7274\n",
      "\n",
      "Epoch 6 Loss: 1.7516\n",
      "Time taken for 1 epoch 26.33 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.7199\n",
      "Epoch 7 Batch 50 Loss 1.7021\n",
      "Epoch 7 Batch 100 Loss 1.6858\n",
      "Epoch 7 Batch 150 Loss 1.6707\n",
      "\n",
      "Epoch 7 Loss: 1.6910\n",
      "Time taken for 1 epoch 40.95 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.6644\n",
      "Epoch 8 Batch 50 Loss 1.6490\n",
      "Epoch 8 Batch 100 Loss 1.6351\n",
      "Epoch 8 Batch 150 Loss 1.6222\n",
      "\n",
      "Epoch 8 Loss: 1.6396\n",
      "Time taken for 1 epoch 26.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.6168\n",
      "Epoch 9 Batch 50 Loss 1.6032\n",
      "Epoch 9 Batch 100 Loss 1.5909\n",
      "Epoch 9 Batch 150 Loss 1.5794\n",
      "\n",
      "Epoch 9 Loss: 1.5949\n",
      "Time taken for 1 epoch 26.25 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.5746\n",
      "Epoch 10 Batch 50 Loss 1.5622\n",
      "Epoch 10 Batch 100 Loss 1.5511\n",
      "Epoch 10 Batch 150 Loss 1.5407\n",
      "\n",
      "Epoch 10 Loss: 1.5547\n",
      "Time taken for 1 epoch 41.08 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 11 Batch 0 Loss 1.5364\n",
      "Epoch 11 Batch 50 Loss 1.5250\n",
      "Epoch 11 Batch 100 Loss 1.5146\n",
      "Epoch 11 Batch 150 Loss 1.5051\n",
      "\n",
      "Epoch 11 Loss: 1.5180\n",
      "Time taken for 1 epoch 26.28 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 12 Batch 0 Loss 1.5011\n",
      "Epoch 12 Batch 50 Loss 1.4904\n",
      "Epoch 12 Batch 100 Loss 1.4806\n",
      "Epoch 12 Batch 150 Loss 1.4716\n",
      "\n",
      "Epoch 12 Loss: 1.4838\n",
      "Time taken for 1 epoch 26.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 13 Batch 0 Loss 1.4679\n",
      "Epoch 13 Batch 50 Loss 1.4576\n",
      "Epoch 13 Batch 100 Loss 1.4483\n",
      "Epoch 13 Batch 150 Loss 1.4397\n",
      "\n",
      "Epoch 13 Loss: 1.4513\n",
      "Time taken for 1 epoch 40.95 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 14 Batch 0 Loss 1.4361\n",
      "Epoch 14 Batch 50 Loss 1.4261\n",
      "Epoch 14 Batch 100 Loss 1.4170\n",
      "Epoch 14 Batch 150 Loss 1.4088\n",
      "\n",
      "Epoch 14 Loss: 1.4200\n",
      "Time taken for 1 epoch 26.15 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 15 Batch 0 Loss 1.4053\n",
      "Epoch 15 Batch 50 Loss 1.3956\n",
      "Epoch 15 Batch 100 Loss 1.3867\n",
      "Epoch 15 Batch 150 Loss 1.3785\n",
      "\n",
      "Epoch 15 Loss: 1.3895\n",
      "Time taken for 1 epoch 26.33 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 16 Batch 0 Loss 1.3751\n",
      "Epoch 16 Batch 50 Loss 1.3655\n",
      "Epoch 16 Batch 100 Loss 1.3568\n",
      "Epoch 16 Batch 150 Loss 1.3488\n",
      "\n",
      "Epoch 16 Loss: 1.3596\n",
      "Time taken for 1 epoch 40.95 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 17 Batch 0 Loss 1.3455\n",
      "Epoch 17 Batch 50 Loss 1.3360\n",
      "Epoch 17 Batch 100 Loss 1.3274\n",
      "Epoch 17 Batch 150 Loss 1.3196\n",
      "\n",
      "Epoch 17 Loss: 1.3302\n",
      "Time taken for 1 epoch 40.95 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 18 Batch 0 Loss 1.3162\n",
      "Epoch 18 Batch 50 Loss 1.3070\n",
      "Epoch 18 Batch 100 Loss 1.2985\n",
      "Epoch 18 Batch 150 Loss 1.2908\n",
      "\n",
      "Epoch 18 Loss: 1.3013\n",
      "Time taken for 1 epoch 26.61 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 19 Batch 0 Loss 1.2875\n",
      "Epoch 19 Batch 50 Loss 1.2784\n",
      "Epoch 19 Batch 100 Loss 1.2701\n",
      "Epoch 19 Batch 150 Loss 1.2625\n",
      "\n",
      "Epoch 19 Loss: 1.2728\n",
      "Time taken for 1 epoch 26.49 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 20 Batch 0 Loss 1.2593\n",
      "Epoch 20 Batch 50 Loss 1.2504\n",
      "Epoch 20 Batch 100 Loss 1.2423\n",
      "Epoch 20 Batch 150 Loss 1.2348\n",
      "\n",
      "Epoch 20 Loss: 1.2449\n",
      "Time taken for 1 epoch 26.31 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 21 Batch 0 Loss 1.2317\n",
      "Epoch 21 Batch 50 Loss 1.2232\n",
      "Epoch 21 Batch 100 Loss 1.2152\n",
      "Epoch 21 Batch 150 Loss 1.2079\n",
      "\n",
      "Epoch 21 Loss: 1.2178\n",
      "Time taken for 1 epoch 26.16 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 22 Batch 0 Loss 1.2049\n",
      "Epoch 22 Batch 50 Loss 1.1966\n",
      "Epoch 22 Batch 100 Loss 1.1890\n",
      "Epoch 22 Batch 150 Loss 1.1820\n",
      "\n",
      "Epoch 22 Loss: 1.1915\n",
      "Time taken for 1 epoch 26.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 23 Batch 0 Loss 1.1790\n",
      "Epoch 23 Batch 50 Loss 1.1711\n",
      "Epoch 23 Batch 100 Loss 1.1637\n",
      "Epoch 23 Batch 150 Loss 1.1569\n",
      "\n",
      "Epoch 23 Loss: 1.1660\n",
      "Time taken for 1 epoch 27.35 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 24 Batch 0 Loss 1.1541\n",
      "Epoch 24 Batch 50 Loss 1.1464\n",
      "Epoch 24 Batch 100 Loss 1.1393\n",
      "Epoch 24 Batch 150 Loss 1.1328\n",
      "\n",
      "Epoch 24 Loss: 1.1416\n",
      "Time taken for 1 epoch 26.40 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 25 Batch 0 Loss 1.1301\n",
      "Epoch 25 Batch 50 Loss 1.1228\n",
      "Epoch 25 Batch 100 Loss 1.1160\n",
      "Epoch 25 Batch 150 Loss 1.1098\n",
      "\n",
      "Epoch 25 Loss: 1.1182\n",
      "Time taken for 1 epoch 26.39 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 26 Batch 0 Loss 1.1072\n",
      "Epoch 26 Batch 50 Loss 1.1002\n",
      "Epoch 26 Batch 100 Loss 1.0938\n",
      "Epoch 26 Batch 150 Loss 1.0878\n",
      "\n",
      "Epoch 26 Loss: 1.0959\n",
      "Time taken for 1 epoch 26.39 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 27 Batch 0 Loss 1.0854\n",
      "Epoch 27 Batch 50 Loss 1.0787\n",
      "Epoch 27 Batch 100 Loss 1.0725\n",
      "Epoch 27 Batch 150 Loss 1.0668\n",
      "\n",
      "Epoch 27 Loss: 1.0745\n",
      "Time taken for 1 epoch 27.19 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 28 Batch 0 Loss 1.0645\n",
      "Epoch 28 Batch 50 Loss 1.0581\n",
      "Epoch 28 Batch 100 Loss 1.0522\n",
      "Epoch 28 Batch 150 Loss 1.0468\n",
      "\n",
      "Epoch 28 Loss: 1.0541\n",
      "Time taken for 1 epoch 26.27 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 29 Batch 0 Loss 1.0445\n",
      "Epoch 29 Batch 50 Loss 1.0386\n",
      "Epoch 29 Batch 100 Loss 1.0330\n",
      "Epoch 29 Batch 150 Loss 1.0279\n",
      "\n",
      "Epoch 29 Loss: 1.0348\n",
      "Time taken for 1 epoch 26.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 30 Batch 0 Loss 1.0258\n",
      "Epoch 30 Batch 50 Loss 1.0201\n",
      "Epoch 30 Batch 100 Loss 1.0148\n",
      "Epoch 30 Batch 150 Loss 1.0099\n",
      "\n",
      "Epoch 30 Loss: 1.0165\n",
      "Time taken for 1 epoch 26.19 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ],
   "metadata": {
    "id": "Qyo2-TkvxijD"
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start=time.time()\n",
    "states=None\n",
    "next_char=tf.constant(['ROMEO'])\n",
    "result=[next_char]\n",
    "states=None\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char,states=one_step_model.generate_one_step(next_char,states=states)\n",
    "  result.append(next_char)\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRUUdWVixex1",
    "outputId": "ae4d4915-c9a0-40f7-b8db-48b3ae23ffcd"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROMEO:\n",
      "The boldness in your weeding king, my mother\n",
      "To slay thyself now! Still till Nor Ille\n",
      "He cometh agreeds; bawd to reprehely me\n",
      "Above a bloody mind. For well I wot.\n",
      "\n",
      "GRUMIO:\n",
      "And ye see there is no frightony than their good seconds\n",
      "cold courtier as she is not univantled,\n",
      "And every thin a dial of you then in France,\n",
      "His youngest daughter, Shepherd's one;\n",
      "I was awake her sighn; like a fish, how here,\n",
      "I bud a lady man as woman\n",
      "For my deserting, Romeo, thence on his treasure,\n",
      "Nothing but so: and to mine honour beast!\n",
      "Besides, his favourive with me in four clothes!\n",
      "Or you have purgest neat, cry 'Who is oft! I have said,\n",
      "His very man would faint a Jack and reason;\n",
      "The love thou said, for much subbling than thou hast evident\n",
      "All that which in their rodes manswer, hath some ill un\n",
      "appleants back against the last with griefs,\n",
      "Making anon with kinning secrect love.\n",
      "\n",
      "KING HENRY VI\n",
      "\n",
      "RIVERS:\n",
      "Madam, his heart as much as in a lovelimy house:\n",
      "It were remain a little helping head.\n",
      "\n",
      "Second Murderer:\n",
      "'Gai \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.616832971572876\n"
     ]
    }
   ]
  }
 ]
}
